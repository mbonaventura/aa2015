{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center><h1>Cap\u00edtulo 2: Busqueda y Testing de clasificadores</h1></center>\n",
      "\n",
      "<h3>Introducci\u00f3n</h3>\n",
      "Existen en la actualidad una gran cantidad m\u00e9todos de aprendizaje autom\u00e1tico que permiten entrenar un algoritmo con instancias de un modelo, de forma que posteriormente este modelo pueda clasificar nuevas instancias. S\u00f3lo tener una idea de cantidad de m\u00e9todos existentes, podemos mencionar algunos de los m\u00e9todos estudiados en la materia: \u00c1rboles de decici\u00f3n, K vecinos mas cercanos, Naive Bayes, SVM, Modelos de regresion, etc. Adicionalmente existen m\u00e9todos que realizan ensambles de otros m\u00e9todos, por ejemplo: Bagging, RandomForest, Boosting, etc. Sklearn trae implementado entre 20-40 m\u00e9todos de clasificaci\u00f3n. Ninguno de estos m\u00e9todos funciona bien para cualquier problema, y la performance va a depender de las caracteristicas particulares del modelo que se quiere explicar. La siguiente figura muestra diferentes metodos de clasificacion para diferentes instancias, donde se puede ver como las clasifica cada uno:\n",
      "\n",
      "<img src=\"files/img/ClasificacionMetodos.png\" /><br>\n",
      "\n",
      "Entonces, \u00bfcomo saber cu\u00e1l de todos estos clasificadores se comportar\u00e1 mejor para nuestro problema? \n",
      "Incluso en el \u00fatopico caso de conocer cual es el mejor m\u00e9todo de clasificacion para este problema, todos los modelos mencionados anteriormente deben instanciarse con diferentes parametros. Estos par\u00e1metros pueden tomar valores discretos o continuos, por lo tanto tampoco sabr\u00edamos cu\u00e1l es el conjunto de valores que produce el mejor clasificador.\n",
      "\n",
      "En este cap\u00edtulo mostraremos la implementacion de una b\u00fasqueda exahusiva en el espacio de posibles clasificadores y sus par\u00e1metros conocido como GridSearch, que describiremos en detalle en la siguiente secci\u00f3n. En esta b\u00fasqueda se realiza N validaciones cruzadas (cross-validation) para cada posibilidad y se selecci\u00f3na la que maximice alguna metrica de performance (presi\u00f3n o curva ROS por ejemplo). Cabe destacar que no es posible que esta b\u00fasqueda cubra el espacio completo de posibilidades, y simplemente se limita a probar con un subconjunto grande de posibilidades. \n",
      "\n",
      "En el cap\u00edtulo anterior se presentaron varios m\u00e9todos para la extracci\u00f3n de atributos de las imagenes. Cada uno de estos m\u00e9todos de extrae una lista de atributos de cada imagen, que se utilizar\u00e1n en este cap\u00edtulo c\u00f3mo los atributos para entrenar y realizar la clasificacion (como se muestra en la figura 1 de la introducci\u00f3n). Esto a su vez agrega una nueva dimensi\u00f3n en el espacio de b\u00fasqueda: cada m\u00e9todo de extracci\u00f3n genera una lista de atributos diferente, \u00bfCu\u00e1l es el mejor set de features?\n",
      "Por lo tanto, para cada una de las posibles features, correremos la b\u00fasqueda exhaustiva de GridSeach con todos los clasificadores ... muchas muchas pruebas!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>B\u00fasqueda Exhaustiva GridSearch</h3>\n",
      "Sklearn implementa una b\u00fasqueda exhaustiva en la clase [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV), que considera todas las combinaciones de los par\u00e1metros especificados para un determinado clasificador. Para cada una de estas combinaciones, GridSearch genera el estimador con los parametros, y luego realiza el t\u00edpico proceso de cross-validation para evaluar su performance: divide las instancias en train y test (cross_validation.train_test_split), entrena el clasificador con las instancias de train (fit) y realiza cross_val_score. A su vez cross_val_score utiliza una cantidad de fold, que para nuestro trabajo fijamos en 5. Tambi\u00e9n hay que seleccionar cual es la metrica que se desea maximizar en esta b\u00fasqueda, que para nuestro trabajo fijamos.\n",
      "\n",
      "Entonces para definir correctamente una b\u00fasqueda exhaustiva en necesario especificar: <br>\n",
      "1. Un estimador. (Ej: RandomForest, SVC, etc)<br>\n",
      "2. El espacio de parametros para realizar la busqueda.<br>\n",
      "3. M\u00e9todo para realizar el cross-validation.<br>\n",
      "4. M\u00e9trica a maximizar<br>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El siguiente extracto de c\u00f3digo muestra como instancias GridSearch para que realize la b\u00fasqueda en 36 combinaciones de par\u00e1metros para el clasificador RandomForest, utilizando 5 folds y maximizando la presici\u00f3n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "parameters = {'n_estimators': [5, 20, 100], 'max_features': ['auto', 5, 20, 100], 'max_depth': [10, 50, 100]}\n",
      "estimator = RandomForestClassifier()\n",
      "cv = 5\n",
      "score = 'presicion'\n",
      "clf = GridSearchCV(estimator, parameters, cv=cv, \n",
      "\t\t                       scoring=score)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "GridSearch resuelve entonces la b\u00fasqueda de los p\u00e1rametros para un determinado clasificador. Sin embargo, para nuestro trabajo necesitamos evaluar multiples clasificadores. La funcion gridSearch en classifier_search.py realiza esta funcionalidad.\n",
      "Este m\u00e9todo toma los nombres de los estimadores que desean ser probados por gridSearch. Para cada uno realiza la b\u00fasqueda exhaustiva, imprime los resultados y se guarda el mejor estimador."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gridSearch(X_train, y_train, estimators, featureSet, n_jobs=1):\n",
      "\tscores = ['precision'] # ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\n",
      "\t\n",
      "\tbestEstimators=[]\n",
      "\tfor estimatorName in estimators:\n",
      "\t\testimator, tuned_parameters = getSearch(estimatorName)\n",
      "\t\t\n",
      "\t\tprint(\"\")\n",
      "\t\tprint (\"# Finding best parameters for %s:\" % estimatorName)\n",
      "\t\tprint (\"----------------------------\")\n",
      "\t\tfor scoreName in scores:\n",
      "\t\t\t# Do the grid search\n",
      "\t\t    clf = GridSearchCV(estimator, tuned_parameters, cv=5, \n",
      "\t\t                       scoring='%s' % scoreName,verbose=10, n_jobs=n_jobs)\n",
      "\t\t    clf.fit(X_train, y_train)\n",
      "\n",
      "\t\t    # Add the best estimator to the result\n",
      "\t\t    bestEstimators.append((clf.best_estimator_, clf.best_score_))\n",
      "\n",
      "\t\t    # save estimator\n",
      "\t\t    saveGridSearchResults(clf, './estimators/%s' % featureSet)  \n",
      "\n",
      "\t\t    print(\"\\tBest parameters: %s\" % clf.best_estimator_)\n",
      "\t\t    print(\"\\tBest score (%s): %s\" % (scoreName, clf.best_score_))\t\n",
      "\t\t    print(\"\")\t    \n",
      "\t\t    \n",
      "\t\t    print(\"Grid scores on development set:\")\n",
      "\t\t    print(\"\")\n",
      "\t\t    for params, mean_score, grid_scores in clf.grid_scores_:\n",
      "\t\t        print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "\t\t              % (mean_score, grid_scores.std() * 2, params))\n",
      "\t\t    print(\"\")\t\t    \n",
      "\treturn bestEstimators"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La definicion de qu\u00e9 espacio de p\u00e1rametros utilizar para cada clasificador, definimos un mapeo entre el nombre del claficador y los parametros que se utilizaran. Esto se muestra a continuaci\u00f3n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Ensambles utilizados</h3>\n",
      "TBC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Otros clasificadores</h3>\n",
      "TBC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Parametrizacion de cada clasificador</h3>\n",
      "TBC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Resultados</h3>\n",
      "Poner aca la tablita que hicimos en google\n",
      "TBC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3> Conclusi\u00f3n</h3>\n",
      "TBC\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}