{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center><h1>Cap\u00edtulo 2: B\u00fasqueda y Testing de Clasificadores</h1></center>\n",
      "\n",
      "<h3>Introducci\u00f3n</h3>\n",
      "Existen en la actualidad una gran cantidad m\u00e9todos de aprendizaje autom\u00e1tico que permiten entrenar un algoritmo con instancias de un modelo, de forma que posteriormente \u00e9ste pueda clasificar nuevas instancias. S\u00f3lo tener una idea de cantidad de m\u00e9todos existentes, podemos mencionar algunos de los estudiados en la materia: \u00c1rboles de decici\u00f3n, K vecinos mas cercanos, Naive Bayes, SVM, Modelos de regresion, etc. Adicionalmente existen m\u00e9todos que realizan ensambles de otros m\u00e9todos, por ejemplo: Bagging, RandomForest, Boosting, etc. SKlearn ofrece implementado entre 20-40 m\u00e9todos de clasificaci\u00f3n. Ninguno de estos m\u00e9todos funciona bien para cualquier problema, y la performance va a depender de las caracteristicas particulares del modelo que se quiere explicar. La siguiente figura muestra diferentes metodos de clasificacion para diferentes instancias, donde se puede ver como las clasifica cada uno:\n",
      "<center>\n",
      "<img src=\"files/img/ClasificacionMetodos.png\" /><br>\n",
      "Fuente: http://scikit-learn.org/stable/auto_examples/classification\n",
      "</center>\n",
      "Entonces, \u00bfC\u00f3mo saber cu\u00e1l de todos estos clasificadores se comportar\u00e1 mejor para nuestro problema? \n",
      "Incluso en el \u00fatopico caso de conocer cual es el mejor m\u00e9todo de clasificacion para este problema, todos los modelos mencionados anteriormente deben instanciarse con diferentes par\u00e1metros. Estos par\u00e1metros pueden tomar valores discretos o continuos, por lo tanto tampoco sabr\u00edamos cu\u00e1l es el conjunto de valores que produce el mejor clasificador.\n",
      "\n",
      "En este cap\u00edtulo mostraremos la implementaci\u00f3n de una b\u00fasqueda exahustiva en el espacio de posibles clasificadores y sus par\u00e1metros conocido como GridSearch, que describiremos en detalle en la siguiente secci\u00f3n. En esta b\u00fasqueda se realiza N validaciones cruzadas (cross-validation) para cada posibilidad y se selecciona la que maximice alguna m\u00e9trica de performance (precision o \u00e1rea bajo la curva ROC, por ejemplo). Cabe destacar que no es posible que esta b\u00fasqueda cubra el espacio completo de posibilidades, y simplemente se limita a probar con un subconjunto grande de variantes. \n",
      "\n",
      "En el cap\u00edtulo anterior se presentaron varios m\u00e9todos para la extracci\u00f3n de atributos de las im\u00e1genes. Cada uno de estos m\u00e9todos extrae una lista de atributos de cada imagen, que se utilizar\u00e1n en este cap\u00edtulo c\u00f3mo los atributos para entrenar y realizar la clasificaci\u00f3n (como se muestra en la Figura 1 de la Introducci\u00f3n). Esto a su vez agrega una nueva dimensi\u00f3n en el espacio de b\u00fasqueda: cada m\u00e9todo de extracci\u00f3n genera una lista de atributos diferente, \u00bfCu\u00e1l es el mejor set de \"features\"? Por lo tanto, para cada una de los posibles atributos, correremos la b\u00fasqueda exhaustiva de GridSeach con todos los clasificadores en varias pruebas... muchas, muchas pruebas!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>B\u00fasqueda Exhaustiva GridSearch</h3>\n",
      "Sklearn implementa una b\u00fasqueda exhaustiva en la clase [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV), que considera todas las combinaciones de los par\u00e1metros especificados para un determinado clasificador. Para cada una de estas combinaciones, GridSearch genera el estimador con los par\u00e1metros, y luego realiza el t\u00edpico proceso de cross-validation para evaluar su performance: divide las instancias en train y test (cross_validation.train_test_split), entrena el clasificador con las instancias de train (fit) y realiza cross_val_score. A su vez cross_val_score utiliza una cantidad de \"folds\", que para nuestro trabajo fijamos en 5. Tambi\u00e9n hay que seleccionar cual es la m\u00e9trica que se desea maximizar en esta b\u00fasqueda, que para nuestro trabajo fijamos en \"Precision\".\n",
      "\n",
      "Entonces para definir correctamente una b\u00fasqueda exhaustiva en necesario especificar: <br>\n",
      "1. Un estimador. (Ej: RandomForest, SVC, etc)<br>\n",
      "2. El espacio de parametros para realizar la busqueda.<br>\n",
      "3. M\u00e9todo para realizar el cross-validation.<br>\n",
      "4. M\u00e9trica a maximizar<br>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El siguiente extracto de c\u00f3digo muestra como instancias GridSearch para que realize la b\u00fasqueda en 36 combinaciones de par\u00e1metros para el clasificador RandomForest, utilizando 5 folds y maximizando la presici\u00f3n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "parameters = {'n_estimators': [5, 20, 100], 'max_features': ['auto', 5, 20, 100], 'max_depth': [10, 50, 100]}\n",
      "estimator = RandomForestClassifier()\n",
      "cv = 5\n",
      "score = 'precision'\n",
      "clf = GridSearchCV(estimator, parameters, cv=cv, \n",
      "\t\t                       scoring=score)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "GridSearch resuelve entonces la b\u00fasqueda de los p\u00e1rametros para un determinado clasificador. Sin embargo, para nuestro trabajo necesitamos evaluar multiples clasificadores. La funcion gridSearch en classifier_search.py realiza esta funcionalidad.\n",
      "Este m\u00e9todo toma los nombres de los estimadores que desean ser probados por gridSearch. Para cada uno realiza la b\u00fasqueda exhaustiva, imprime los resultados y se guarda el mejor estimador."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gridSearch(X_train, y_train, estimators, featureSet, n_jobs=1):\n",
      "\tscores = ['precision'] # ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\n",
      "\t\n",
      "\tbestEstimators=[]\n",
      "\tfor estimatorName in estimators:\n",
      "\t\testimator, tuned_parameters = getSearch(estimators[estimatorName])\n",
      "\t\t\n",
      "\t\tprint(\"\")\n",
      "\t\tprint (\"# Finding best parameters for %s:\" % estimatorName)\n",
      "\t\tprint (\"----------------------------\")\n",
      "\t\tfor scoreName in scores:\n",
      "\t\t\t# Do the grid search\n",
      "\t\t    clf = GridSearchCV(estimator, tuned_parameters, cv=5, \n",
      "\t\t                       scoring='%s' % scoreName,verbose=10, n_jobs=n_jobs)\n",
      "\t\t    clf.fit(X_train, y_train)\n",
      "\n",
      "\t\t    # Add the best estimator to the result\n",
      "\t\t    bestEstimators.append((clf.best_estimator_, clf.best_score_))\n",
      "\n",
      "\t\t    print(\"\\tBest parameters: %s\" % clf.best_estimator_)\n",
      "\t\t    print(\"\\tBest score (%s): %s\" % (scoreName, clf.best_score_))\t\n",
      "\t\t    print(\"\")\t    \n",
      "\t\t    \n",
      "\t\t    print(\"Grid scores on development set:\")\n",
      "\t\t    print(\"\")\n",
      "\t\t    for params, mean_score, grid_scores in clf.grid_scores_:\n",
      "\t\t        print(\"%0.3f (+/-%0.03f) for %r\"\n",
      "\t\t              % (mean_score, grid_scores.std() * 2, params))\n",
      "\t\t    print(\"\")\t\t    \n",
      "\treturn bestEstimators"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "La definici\u00f3n de qu\u00e9 espacio de p\u00e1rametros utilizar para cada clasificador, definimos un mapeo entre el nombre del claficador y los par\u00e1metros que se utilizar\u00e1n. Esto se muestra a continuaci\u00f3n:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimators[\"SVC\"] = (\n",
      "\t\t\t\t\t\t\tSVC(C=1), \n",
      "\t\t\t\t\t\t     #[{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 2], 'C': [1, 10, 100, 1000]},\n",
      "\t                         #{'kernel': ['linear'], 'C': [0.0025, 1, 5, 100, 1000]}]\t\n",
      "\t                         [{'kernel': ['rbf'], 'gamma': [1e-3], 'C': [1, 100]},\n",
      "\t                         {'kernel': ['linear'], 'C': [1, 5]}]\t\n",
      "\t                    )\n",
      "\testimators[\"RandomForest\"] = (\n",
      "\t\t\t\t\t\t\t\t   RandomForestClassifier(), \n",
      "\t\t\t\t\t\t           [{'n_estimators': [5, 20, 100], 'max_features': ['auto', 5, 20, 100], 'max_depth': [10, 50, 100]}]\n",
      "\t\t\t\t\t\t           #[{'n_estimators': [20], 'max_features': ['auto', 5, 20, 100, 200], 'max_depth': [10, 100, 300]}]\n",
      "\t\t\t\t\t\t         )\n",
      "\testimators[\"ExtraTrees\"] = (\n",
      "\t\t\t\t\t\t\t\tExtraTreesClassifier(),\n",
      "\t\t\t\t\t\t\t\t[{'n_estimators': [5, 20, 100],  'max_features':['auto', 1, 5, 10], 'max_depth': [None, 5, 10], 'min_samples_split':[1], 'random_state':[0]}]\n",
      "\t\t\t\t\t\t\t)\n",
      "\testimators[\"LogisticRegression\"] = (\n",
      "\t\t\t\t\t\t\t\t   \t\t\tLogisticRegression(), \n",
      "\t\t\t\t\t\t           \t\t\t[{'penalty': ['l2'], 'C': [0.001, 0.01, 0.005, 0.0005, 0.0001]}]\n",
      "\t\t\t\t\t\t         \t\t)\n",
      "\testimators[\"DecisionTree\"] = (\n",
      "\t\t\t\t\t\t\t\t   tree.DecisionTreeClassifier(), \n",
      "\t\t\t\t\t\t           [{'max_depth': [5, 10], 'max_leaf_nodes': [50, 100]}]\n",
      "\t\t\t\t\t\t         )\n",
      "\testimators[\"AdaBoost\"] = (\n",
      "\t\t\t\t\t\t\t\tAdaBoostClassifier(), \n",
      "\t\t\t\t\t\t        [{'n_estimators': [20, 50, 100], 'algorithm': ['SAMME.R']}]\n",
      "\t\t\t\t\t\t     )\n",
      "\testimators[\"GradientBoosting\"] = (\n",
      "\t\t\t\t\t\t\t\tGradientBoostingClassifier(), \n",
      "\t\t\t\t\t\t        [{'n_estimators': [20, 100, 150], 'learning_rate': [1.0, 0.1], 'max_depth':[3, 10], 'loss':['deviance']}]\n",
      "\t\t\t\t\t\t     )\t\n",
      "\testimators[\"GaussianNB\"] = (\n",
      "\t\t\t\t\t\t\t\tGaussianNB(), \n",
      "\t\t\t\t\t\t        [{}]\n",
      "\t\t\t\t\t\t     )\n",
      "\testimators[\"Bagging\"] = (\n",
      "\t\t\t\t\t\t\t\tBaggingClassifier(),\n",
      "\t\t\t\t\t\t\t\t[{'base_estimator': [KNeighborsClassifier(), tree.DecisionTreeClassifier()],\n",
      "\t\t\t\t\t\t\t\t  'n_estimators': [5, 10], 'max_features':[0.5], 'max_samples':[0.5]}]\n",
      "\t\t\t\t\t\t\t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Ensambles utilizados</h3>\n",
      "Como se puede observar, decidimos probar la performance de clasificaci\u00f3n utilizando 9 clasificadores. A su vez, 5 de estos clasificadores son ensambles:<br>\n",
      "1- <b>[Bagging](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier):</b> BaggingClassifier utiliza clasificadores base (en nuestro caso KNeighborsClassifier y DecisionTreeClassifier) y busca reducir la varianza de estos estimadores creando conjuntos aleatorios y agregando la prediccion individual <br>\n",
      "2- <b>[GaussianNB](http://scikit-learn.org/stable/modules/gaussian_process.html): </b> Corresponde a un clasificador Bayesiano del que no se esperaba a priori alta performance, pero se teste\u00f3 de todos modos.<br>\n",
      "3- <b>[DecisionTree](http://scikit-learn.org/stable/modules/tree.html#classification):</b> Implementaci\u00f3n de \u00c1rboles de Decisi\u00f3n<br>\n",
      "4- <b>[GradientBoosting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier): </b>Es un m\u00e9todo de ensamble de clasificadores \"d\u00e9biles\" (weak), en general, \u00c1rboles de Decisi\u00f3n<br>\n",
      "5- <b>[AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier): </b>Un cl\u00e1sico algoritmo de boosting, similar al anterior pero que utiliza una funci\u00f3n de p\u00e9rdida diferente. <br>\n",
      "6- <b>[LogisticRegression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression: </b><br>\n",
      "7- <b>[RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier): </b>utiliza una t\u00e9cnica de perturbaci\u00f3n y combinaci\u00f3n introduciendo aleratoriedad en la construcci\u00f3n de  \u00e1rboles de decici\u00f3n  <br>\n",
      "8- <b>[ExtraTrees](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier): </b>Similarmente como RandomForest trabaja con \u00e1rboles de decici\u00f3n aleatorios, pero en este caso con mayor aleatoriedad. Respecto a RandomForest suele reducir la varianza pero aumentando el bias<br>\n",
      "9- <b>[SVC](http://scikit-learn.org/stable/modules/svm.html): </b>Implementaci\u00f3n de Support Vector Machines<br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Resultados</h3>\n",
      "Para la ejecuci\u00f3n de esta b\u00fasqueda exhaustiva utilizamos un servidor con 24 procesadores. De esta forma pudimos sacar provecho de la opcion de paralelizaci\u00f3n que est\u00e1 implementada para gridsearch en sklearn (mediante el parametro n_jobs de GridSearchCV). A pesar de contar con un servidor potente y una gran cantidad de cores, la corrida para cada una de las features tarda alrededor de 4-5 horas.\n",
      "Para realizar correctamente las pruebas, el metodo de gridsearch se ejecuta con el 20% de las imagenes totales. Esto es para una vez encontrados los mejores clasificadores, verificar si la precis\u00f3n esperada coincide al predecir ese 20% de imagenes restantes. Esto se realiza en el cap\u00edtuo siguente y utilizando el clasificador Voting.\n",
      "\n",
      "A continuaci\u00f3n se muestran los resultados obtenidos para los clasificadores no ensambles para todas las features. Se muestra la presici\u00f3n esperada (calculada con cross-validation) y la desviaci\u00f3n estandard de cada clasificador."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/img/tablaFeaturesVsNoEnsamble.png\" /><br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Como se puede observar, los modelos que peor se comportan son DecisionTree y Gaussian, con valores de precision inferiores a 60%. Por otro lado, se puede ver que el set de features de histograma de colores (colorhist) se comporta mal con todos estos clasificadores. \n",
      "\n",
      "Por otro lado, se puede deducir de esta tabla que los features normalizados en general se comportan mejor que su contrapartida sin normalizar. Tambien sorprende la performance de los clasificadores LogisticRegression y SVM. En particular estos clasificadores utilizando el set de features de surf con clusters de 300 y normalizados, alcanza una presici\u00f3n mayor al 73%."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n se muestra la misma tabla pero en este caso para los clasificadores ensambles. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/img/tablaFeaturesVsEnsamble.png\" /><br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Aqui se puede observar tambi\u00e9n que las features de SURF generan una buena performance general con todos los clasificadores. En particular SURF con 300 clusters y normalizados parece ser el mejor set de features. En cuanto a las features de texturas, la mejor pareceria ser LBP, pero la performance es baja (cercana al 60%).\n",
      "\n",
      "Los clasificadores de ensable parecen tener en lineas generales una mejor performance. Sin embargo, el mejor clasificador sigue siendo LogisticRegression y SVM. Dentro de los clasificadores de ensambles el mejor es GradientBoosting pero con peque\u00f1as diferencias respecto a RandomForest y AdaBoost."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cabe notar que en estas tablas se muestra la precisi\u00f3n para el mejor set de par\u00e1metros de cada clasificador. En las pruebas realizadas de gridsearch, guardamos en disco tanto la precisi\u00f3n, los p\u00e1rametros y el clasificador ya entrenado. Esto se explica con mayor detalle en el siguiente cap\u00edtulo. Para mayor informaci\u00f3n se puede consultar la siguiente tabla, donde ademas de la presicion inclu\u00edmos los par\u00e1metros que generaron esa mejor precisi\u00f3n: [tabla de features vs clasificadores](https://docs.google.com/spreadsheets/d/1QUoA3orYAZi4s2TOtND3CdrLgHBIyWlrHAVUxew6LaY/edit#gid=797229090&vpid=A1)\n",
      "\n",
      "Por otro lado, se puede ver que algunas celdas no estan completas. Tambien por ejemplo falta correr las features de hara en los primeros clasificadores. Esto se debe a que cuando lanzamos todas las ejecuciones en el servidor no todas terminaron satisfactoriamente. Algunas ya fueron arregladas, y seguiremos realizando pruebas hasta el d\u00eda de la competencia con m\u00e1s clasificadores y probando con nuevos sets de par\u00e1metros. Los resultados los iremos actualizando en la tabla compartida de google mencionada anteriormente."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3> Conclusi\u00f3n</h3>\n",
      "En esta secci\u00f3n vimos el proceso que utilizamos para poner a prueba la gran cantidad de clasificadores y espacio de parametros de cada uno, para encontrar los mejores clasificadores y su precisi\u00f3n esperada. Vimos que los mejores clasificadores con el mejor set de features posee una presici\u00f3n superior al 75%.\n",
      "En el siguiente capitulo veremos como puede utilizarse el clasificador de Voting, para mejorar aun mas la performance, utilizando no el mejor m\u00e9todo encontrado sino una conjuncion de loS mejoreS m\u00e9todoS. Tambi\u00e9n veremos como guardar los clasificadores una vez entrenados para ahorrar tiempo de procesamiento a la hora de la competencia.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}