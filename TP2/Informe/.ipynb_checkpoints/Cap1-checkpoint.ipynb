{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "<center><h1>Cap\u00edtulo 1: Extracci\u00f3n de atributos de las imagenes</h1></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Introducci\u00f3n</h3>\n",
      "\n",
      "La selecci\u00f3n y extracci\u00f3n de atributos (features) es una de las tareas m\u00e1s complejas en todo el proceso de construcci\u00f3n de la soluci\u00f3n de clasificaci\u00f3n. Por un lado, se requiere determinar aquellas caracter\u00edsticas que resulten apropiadas al tipo de objetos a clasificar y - por el otro - provean informaci\u00f3n relevante que permita discriminar de mejor manera tales objetos. Adem\u00e1s, el conjunto de atributos debe contener aquellos que sean menos sensitivos a la variabilidad de los objetos de entrada y limitados en n\u00famero para permitir su extracci\u00f3n y entrenamiento del clasificador en un tiempo razonable (esto \u00faltimo depende, en parte, del problema). El objetivo principal en esta instancia es extraer el conjunto de atributos con el menor n\u00famero de elementos (menor dimensionalidad) que maximiza la performance del clasificador. Eventualmente, se puede plantear un tradeoff entre performance y tiempo de entrenamiento y test al momento de seleccionar los atributos.\n",
      "\n",
      "En general, durante este proceso, los atributos son extraidos en forma de vectores de caracter\u00edsticas (feature vectors) que se constituye - de alguna manera - en la \"identidad\" de cada objeto. Para el caso particular del reconocimiento de im\u00e1genes se han propuesto y evaluado m\u00faltiples enfoques, con mayor o menor nivel de complejidad. Incluso, en alguno se realiza un preprocesamiento espec\u00edfico a las im\u00e1genes que requiere de un conocimiento detallado del dominio y de las t\u00e9cnicas de manejo de im\u00e1genes.\n",
      "\n",
      "En este trabajo se tomaron atributos de las im\u00e1genes ampliamente utilizados que tratan de capturar diferentes aspectos de \u00e9stas y que se dividen en dos grupos:\n",
      "<ul>\n",
      " <li> Atributos de color\n",
      " <li> Atributos de textura\n",
      "</ul>\n",
      "\n",
      "Los atributos de color tratan de capturar la distribuci\u00f3n de los colores (o escala de grises) que aparecen en las im\u00e1genes, mientras que los atributos de textura intentan capturar la interacci\u00f3n entre los colores de algunos p\u00edxeles en regiones de la imagen. Aplicado al problema de clasificar im\u00e1genes, y en particular al caso de distinguir im\u00e1genes de \"perros\" y \"gatos\", este es un enfoque estandard que propone definir la \"identidad\" de cada imagen desde dos perspectivas diferentes. En total, se extrajeron cinco conjuntos de atributos (2 de color y 3 de textura) los cuales luego fueron combinados de diferentes maneras para obtener el conjunto final utilizado en la evaluaci\u00f3n."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Atributos de Color</h3>\n",
      "\n",
      "Como se mencion\u00f3, los atributos de color se basan en las distribuciones de los valores de los p\u00edxeles de las im\u00e1genes en un espacio de colores (por ejemplo, RGB) y en este trabajo se utilizaron los siguientes:\n",
      "\n",
      "<b>Histograma de Colores:</b> Es una representaci\u00f3n de la distribuci\u00f3n de los colores en una imagen, es decir, la cantidad de p\u00edxeles que existen en determinados rangos de colores (en RGB) dentro del abanico de posibilidades. Este enfoque extrae la distribuci\u00f3n estad\u00edstica que caracteriza la proporci\u00f3n de diferentes grupos de colores sin tener en cuenta su ubicaci\u00f3n dentro de la imagen. Por ejemplo, en im\u00e1genes en el espacio de colores RGB las intensidades se agrupan en rangos para cada canal, generando un conjunto limitado de atributos.\n",
      "\n",
      "La extracci\u00f3n del histograma de colores se realiz\u00f3 utilizando la librer\u00eda de procesamiento de im\u00e1genes Mahotas, como se muestra a continuaci\u00f3n."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image = mahotas.imread(filename, as_grey=False)\n",
      "color_hist, bins = np.histogram(image.flatten(), 256, [0, 256])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "En este caso, se lee la imagen a una matriz que luego es aplanada (\"flatten\") para obtener un conjunto fijo de \"bins\" de 256 elementos (obviamente, todos los histrogramas deben poseer la mismas cantidad de elementos). Esta idea permite \"estirar\" el histograma para ambos extremos (como se muestra en la imagen) aplanando la curva de la distribuci\u00f3n de las intensidades. <br>\n",
      "<center>\n",
      "<img src=\"files/img/flatten.png\" /><br>\n",
      "Fuente: http://docs.opencv.org/master/d5/daf/tutorial_py_histogram_equalization.html#gsc.tab=0\n",
      "</center><br>\n",
      "Este tratamiento funciona como una forma de equalizaci\u00f3n que normalmente mejora el contraste de la imagen. El histograma de color (color_hist) es el primer conjunto de atributos y consta de 256 elementos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Histograma de Grises:</b> La idea en este caso es similar a la anterior pero se aplica para una im\u00e1gen convertida a escala de grises. Se construye el mismo histograma pero en este caso formado por las intensidades en la escala mencionada por lo que en la literatura se lo menciona tambi\u00e9n como \"Histograma de Intensidad\". Se le aplica el mismo tratamiento que en el caso anterior utilizando la misma librer\u00eda. El \u00fanico cambio en la conversi\u00f3n de la imagen al momento de cargarla. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "image = mahotas.imread(filename, as_grey=True)\n",
      "gray_hist, bins = np.histogram(image.flatten(), 256, [0, 256])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " El histograma de grises (gray_hist) es el segundo conjunto de atributos y consta tambi\u00e9n de 256 elementos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Atributos de Textura</h3>\n",
      "\n",
      "En general, el t\u00e9rmino textura hace referencia a la forma en que se encuentran relacionados los elementos que constituyen un material. En el caso de los objetos f\u00edsicos, \u00e9stas se reconocen por el sentido del tacto principalmente (y la vista, complementariamente). Tambi\u00e9n se la utiliza en el contexto de la pintura, las ciencias e incluso la m\u00fasica (aqu\u00ed se incorpora el sentido del oido).\n",
      "\n",
      "En el caso de la im\u00e1genes digitales, se refiere la las interrelaciones espaciales entre los p\u00edxeles. \u00c9stas permiten distinguir areas de los objetos en las im\u00e1genes con color similar (por ejemplo, el cielo, el cesped). Habitualmente, para realizar este an\u00e1lisis la imagen debe ser segmentada en un n\u00famero que sectores que depende del algoritmo a utilizar. En este trabajo se utilizaron tres enfoques dentro de los considerados de \"extracci\u00f3n de textura\" que intentan mejorar la identidad de las im\u00e1genes para el proceso de clasificaci\u00f3n posterior. Los atributos de textura se clasican en globales y locales, donde los primeros corresponden al an\u00e1lisis de toda la imagen mientras que los segundos describen porciones espec\u00edficas de \u00e9sta. En este trabajo se utilizan ambos enfoques.\n",
      "\n",
      "\n",
      "<b>Atributos Haraclick</b>: Estos atributos fueron propuestos por Robert Haraclick sobre la base del an\u00e1lisis de una im\u00e1gen en escala de grises (corresponden a atributos globales). Se basa en calcular la matrix de co-ocurrencia ($G$), de dimensi\u00f3n $N_g$, donde $N_g$ representa la cantidad de grises de la escala de la imagen. \n",
      "<center>\n",
      "<img src=\"files/img/hara_matG.gif\" /><br>\n",
      "Fuente: http://murphylab.web.cmu.edu/publications/boland/boland_node26.html\n",
      "</center><br>\n",
      "\n",
      "\n",
      "Cada elemento $e_{ij}$ de la matriz corresponde a la cuenta del n\u00famero de veces que un pixel con valor $i$ es adyancente a un pixel con valor $j$. Luego de normalizar, cada entrada es considerada como la probabilidad de que un pixel con valor $i$ sea adyacente a uno con valor $j$. La idea de adyacencia se puede definir en cuatro direcciones (arriba, abajo, derecha, izquierda) como se muestra en la siguiente figura, por lo que se calculan cuatro matrices.\n",
      "\n",
      "<center>\n",
      "<img src=\"files/img/hara_directions.gif\" /><br>\n",
      "Fuente: http://murphylab.web.cmu.edu/publications/boland/boland_node26.html\n",
      "</center><br>\n",
      "\n",
      "Haraclick defini\u00f3 14 atributos a calcular sobre esta matriz (por detalles y f\u00f3rmulas, por favor revisar: http://murphylab.web.cmu.edu/publications/boland/boland_node26.html) aunque habitualmente se implementan los 13 primeros. La librer\u00eda Mahotas implementa la extracci\u00f3n de los atributos de textura de Haraclick como:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hara_feat = mahotas.features.haralick(image).mean(0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El vector Haraclick (hara_feat) es el tercer conjunto de atributos extraido y consta, como se mencion\u00f3 de 13 elementos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Local Binary Patterns</b>: Este es atributo que anaiza regiones de la imagen y se basa en analizar un sector alrededor de un determinado pixel (de ah\u00ed su denominaci\u00f3n de 'local') que se toma como centro (en la imagen, el pixel en rojo). \n",
      "\n",
      "<center>\n",
      "<img src=\"files/img/lbp.png\" /><br>\n",
      "Fuente: http://mahotas.readthedocs.org/en/latest/lbp.html\n",
      "</center><br>\n",
      "\n",
      "Luego, se define una serie de puntos a una distancia $r$ (radio) del centro y se calcula un patr\u00f3n comparando el valor del los puntos en la circunferencia respecto del centro y estableciendo si \u00e9ste es mayor (o no, por ello es 'binario'), generando as\u00ed diferentes secuencias. Por ejemplo, suponiendo que se tienen 6 puntos de los cuales solo el primero y el \u00faltimo poseen un valor mayor al punto central, generan un patr\u00f3n como: 100001. Luego, se construye un histograma con las cantidades de cada patr\u00f3n encontrado en la imagen. Esta t\u00e9cnica de extracci\u00f3n de atributos puede ser muy \u00fatil aunque hay que ser cuidadosos con la elecci\u00f3n del n\u00famero de p\u00edxeles ($p$) a analizar porque expande el vector de atributos de la imagen considerablemente.\n",
      "\n",
      "En este caso, la extracci\u00f3n se realiz\u00f3 como:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "radius = 3\n",
      "points = 4 * radius\n",
      "lbp_hist = lbp.lbp(image, radius, points, ignore_zeros=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los valores de $radius$ y $points$ se establecieron siguiendo ejemplos encontrados en la literatura y seleccionando una cantidad de atributos controlada. Para este caso, el vector LBP (lbp_hist) es el cuarto conjunto de atributos extraido y consta de 352 elementos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>SURF (Speeded-Up Robust Features):</b> Se trata tambi\u00e9n de un algoritmo de an\u00e1lisis local. Esta estrategia se basa en detectar puntos de inter\u00e9s en una imagen y luego describirlos como atributos. Utiliza la distribuci\u00f3n de intensidad del contenido dentro del punto de inter\u00e9s de los puntos vecinos, bas\u00e1ndose en el \u00e1rea que rodea al punto de inter\u00e9s. Esto genera un vector descriptor para cada uno de estos puntos. SURF est\u00e1 basado en SIFT (Scale-invariant Feature Transform) que asegura que los puntos de inter\u00e9s son invariantes en el escalado. La siguente imagen muestra un ejemplo de los puntos seleccionados.\n",
      "<center>\n",
      "<img src=\"files/img/surf_example.png\" /><br>\n",
      "Fuente: http://mahotas.readthedocs.org/en/latest/surf.html\n",
      "</center><br>\n",
      "La idea fundamental es transformar la imagen en coordenadas mediante una t\u00e9cnica llamada multi-resoluci\u00f3n. Consiste en hacer una r\u00e9plica de la imagen original de forma Piramidal (Gaussiana o Laplaciana) que permite obtener im\u00e1genes del mismo tama\u00f1o pero con el ancho de banda reducido, generando un efecto de borrosidad sobre la imagen original (Scale-Space) que posee la propiedad mencionada de ser invariante con el escalado. \n",
      "\n",
      "Como cada imagen puede tener un n\u00famero variable de puntos de inter\u00e9s la estrategia que se utiliza para derivar atributos para cada una es realizar un clustering de todos los puntos y luego construir vectores de atributos de acuerdo a la cantidad de puntos que cada imagen tiene en cada cluster. Aqu\u00ed, el n\u00famero de clusters a utilizar ($k$) tambi\u00e9n es un par\u00e1metro a considerar. En Mahotas, la extracci\u00f3n de puntos SURF se realiza como:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "surf_features = surf.surf(image)[:, 5:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Los primeros atributos corresponden a informaci\u00f3n complementaria de la detecci\u00f3n del punto (posici\u00f3n y escala) por lo que no se utiliza, quedando solo los descriptores (a partir de la posici\u00f3n 5). Luego de obtenidos todos los descriptores de todas las im\u00e1genes, se realiza el clustering (se seleccion\u00f3 el algoritmo de k-means) y se asignan los histogramas correspondientes a cada imagen como:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for imagename in surf_all_hist:\t\t\t\t\n",
      "    all_hists.append(surf_all_hist[imagename])\n",
      "\t#\n",
      "    X_train_surf_features = np.concatenate(all_hists)\n",
      "\t#\t\t\n",
      "\tprint 'Clustering', len(X_train_surf_features), 'features (k=' + str(n_clusters) + ')'\n",
      "\testimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
      "\testimator.fit_transform(X_train_surf_features)\n",
      "\t#\t\n",
      "\tfinal_features = {}\n",
      "\tfor imagename in surf_all_hist:\n",
      "\t\tinstance = surf_all_hist[imagename]\n",
      "\t\t#\n",
      "\t\tclusters = estimator.predict(instance)\n",
      "\t\tfeatures = np.bincount(clusters)\n",
      "\t\t#\n",
      "\t\tif len(features) < n_clusters:\n",
      "\t\t\tfeatures = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "\t\t#print features\n",
      "\t\t#\t\t\n",
      "\t\tsurf_features[imagename] = features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Para este paso se probaron diferentes valores del par\u00e1metro $k$ (50, 100, 200, 300), resultando el \u00faltimo $k=300$ el que obtuvo la mejor performance. Por lo tanto, histograma de puntos SURF (surf_features) es el quinto conjunto de atributos y consta de 300 elementos."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Imagenes utilizadas para el entrenamiento</h3>\n",
      "TBC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Performance</h3>\n",
      "\"pruebas   de   eficiencia\"\n",
      "\n",
      "TBC"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Conclusi\u00f3n</h3>\n",
      "TBC"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}