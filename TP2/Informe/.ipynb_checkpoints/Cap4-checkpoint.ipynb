{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center><h1>Cap\u00edtulo 4: Clasificaci\u00f3n de Nuevas Instancias</h1></center>\n",
      "\n",
      "<h3>Introducci\u00f3n</h3>\n",
      "\n",
      "En esta secci\u00f3n se describe el proceso completo de c\u00f3mputo de atributos, entrenamiento del clasificador y test de performance. La idea general es tener los atributos de las im\u00e1genes de entramiento previamente computados. Esta tarea se realiz\u00f3 off-line, gener\u00e1ndose 5 archivos de atributos (cada uno con las 25.000 instancias).\n",
      "\n",
      "Luego, las \n",
      "\n",
      "A continuaci\u00f3n se presenta en c\u00f3digo completo con comentarios sobre su uso.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Carga de los atributos pre-computados</h3>\n",
      "\n",
      "En primer lugar se importan las librar\u00edas utilizar y se definen algunas funciones necesarias."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import json\n",
      "import glob\n",
      "import argparse\n",
      "import numpy as np\n",
      "#\n",
      "import mahotas.features\n",
      "from mahotas import lbp\n",
      "from mahotas.features import surf\n",
      "#\n",
      "from sklearn import decomposition\n",
      "from sklearn.cluster import MiniBatchKMeans\n",
      "from sklearn.metrics import *\n",
      "from sklearn import cross_validation\n",
      "from sklearn.svm import SVC\n",
      "from sklearn import linear_model\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import VotingClassifier\n",
      "\n",
      "#---------------------------------------------------------------------------------------\n",
      "def norm_hist(hist):\n",
      "\treturn [float(i)/sum(hist) for i in hist]\n",
      "#---------------------------------------------------------------------------------------\n",
      "def clusterSurfFeatures(surf_all_hist, n_clusters):\n",
      "\t#\n",
      "\tall_hists = []\n",
      "\tfor imagename in surf_all_hist:\t\t\n",
      "\t\tall_hists.append(surf_all_hist[imagename])\n",
      "\t#\n",
      "\tX_train_surf_features = np.concatenate(all_hists)\n",
      "\t#\t\t\n",
      "\tprint 'Clustering', len(X_train_surf_features), 'features (k=' + str(n_clusters) + ')'\n",
      "\testimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
      "\testimator.fit_transform(X_train_surf_features)#\t\n",
      "\tfinal_features = {}\n",
      "\tfor imagename in surf_all_hist:\n",
      "\t\tinstance = surf_all_hist[imagename]\n",
      "\t\t#\n",
      "\t\tclusters = estimator.predict(instance)\n",
      "\t\tfeatures = np.bincount(clusters)\n",
      "\t\t#\n",
      "\t\tif len(features) < n_clusters:\n",
      "\t\t\tfeatures = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
      "\t\t#print features\n",
      "\t\t#\t\t\n",
      "\t\tfinal_features[imagename] = features\t\t\n",
      "\treturn final_features\n",
      "\t\n",
      "#---------------------------------------------------------------------------------------\n",
      "def number_of_pc(variance_ratio, threshold):\n",
      "\ti = 0\n",
      "\tvr = variance_ratio[0]\t\n",
      "\twhile vr < threshold:\n",
      "\t\ti = 1\n",
      "\t\tvr+=variance_ratio[i]\n",
      "\treturn (i+1)\n",
      "#---------------------------------------------------------------------------------------\n",
      "def do_pca(data):\n",
      "\tpca = decomposition.PCA(whiten=True)\n",
      "\tpca.fit(data)\n",
      "\tt_data = pca.transform(data)\n",
      "\tnp_data = np.asarray(t_data)\n",
      "\tX = np_data[:,0:20] \t\t# Retorno las primeras \"nopc\", que explican el 0.99 de la varianza.\n",
      "\treturn X\n",
      "\n",
      "#---------------------------------------------------------------------------------------\n",
      "def json2FeaturesMap(data):\n",
      "\ttmp = {}\t\n",
      "\tfor img in data:\n",
      "\t\ttmp[img] = map(float, data[img].split(';'))\n",
      "\treturn tmp, data.keys()\n",
      "#\n",
      "#---------------------------------------------------------------------------------------\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "El programa principal (cats-and-dogs-finder.py) recibe como par\u00e1metros los diferentes archivos de atributos y dos flags que determinan si se aplica PCA y/o normalizaci\u00f3n (como se describi\u00f3 en el cap\u00edtulo anterior). La idea es calcular para las nuevas instancias solo los atributos con los que se desee trabajar en una corrida puntual, a saber:\n",
      "\n",
      "<pre>\n",
      "usage: cats-and-dogs-finder.py [-h] -t TRAININGSETPATH [-fc FEATURESCOLOR]\n",
      "                               [-fg FEATURESGRAY] [-fh FEATURESHARA]\n",
      "                               [-fl FEATURESLBP] [-fs FEATURESSURF]\n",
      "                               [--pca] [--norm]    \n",
      "</pre>\n",
      "\n",
      "por ejemplo, \n",
      "<pre>\n",
      "$ python cats-and-dogs-finder.py -t img2/ -fg features/features-grayhist.json -fs features/features-surf.json-c300.json\n",
      "</pre>\n",
      "\n",
      "lee las im\u00e1genes a clasificar del directorio img2 y carga los atributos Histograma de Grises (features-grayhist.json) y SURF (features-surf.json-c300.json, en este caso con 300 clusters)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main\n",
      "#---------------------------------------------------------------------------------------\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument(\"-fc\", \"--featuresColor\",  help=\"Color Features File\", default=\"\")\n",
      "parser.add_argument(\"-fg\", \"--featuresGray\",   help=\"Color Features File\", default=\"\")\n",
      "parser.add_argument(\"-fh\", \"--featuresHara\",   help=\"Haraclick Features File\", default=\"\")\n",
      "parser.add_argument(\"-fl\", \"--featuresLbp\",    help=\"LBP Features File\", default=\"\")\n",
      "parser.add_argument(\"-fs\", \"--featuresSurf\",   help=\"SURF Features File\", default=\"\")\n",
      "parser.add_argument('--norm', action='store_true')\n",
      "parser.add_argument('--pca', action='store_true')\n",
      "args = parser.parse_args()\n",
      "\n",
      "#--------------------------------------------------------------------------------------\n",
      "# Cargo los parametros para el modelo\n",
      "train_color_features = {}\n",
      "train_gray_features = {}\n",
      "train_hara_features = {}\n",
      "train_lbp_features = {}\n",
      "train_surf_features = {}\n",
      "#\n",
      "imagename_list = []\n",
      "if (args.featuresColor != \"\"):\t\t\n",
      "\ttrain_color_features, imagename_list = json2FeaturesMap(json.load(open(args.featuresColor)))\n",
      "\tprint \"[TRAIN] 'Color' features loaded!\"\n",
      "if (args.featuresGray != \"\"):\n",
      "\ttrain_gray_features, imagename_list = json2FeaturesMap(json.load(open(args.featuresGray)))\n",
      "\tprint \"[TRAIN] 'Gray' features loaded!\"\n",
      "if (args.featuresHara != \"\"):\t\t\n",
      "\ttrain_hara_features, imagename_list = json2FeaturesMap(json.load(open(args.featuresHara)))\n",
      "\tprint \"[TRAIN] 'Haraclick' features loaded!\"\n",
      "if (args.featuresLbp != \"\"):\t\t\n",
      "\ttrain_lbp_features, imagename_list = json2FeaturesMap(json.load(open(args.featuresLbp)))\n",
      "\tprint \"[TRAIN] 'LBP' features loaded!\"\n",
      "if (args.featuresSurf != \"\"):\t\t\n",
      "\ttrain_surf_features, imagename_list = json2FeaturesMap(json.load(open(args.featuresSurf)))\n",
      "\tprint \"[TRAIN] 'SURF' features loaded!\"\n",
      "#\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A continuaci\u00f3n, las atributos seleccionados son combinadas primero en los grupos de \"Atributos de Color\" (X1_data) y \"Atributos de Textura\" (X2_data) y se aplica PCA (si corresponde). Finalmente, se concatenan en un \u00fanico vector de caracter\u00edsticas por imagen (X_data) "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y_data = []\n",
      "X_data = []\n",
      "X1_data = []\n",
      "for img in imagename_list:\n",
      "\ttarget = 1 if 'cat' in img else 0\t\t\n",
      "\tY_data.append(target)\n",
      "\t#\n",
      "\tthis_color_features = []\n",
      "\t# Features sobre 'colores': histogramas de grises y colores \n",
      "\tthis_color_features.extend(train_color_features[img]) if (args.featuresColor != \"\")\telse \"\"\n",
      "\tthis_color_features.extend(train_gray_features[img])  if (args.featuresGray != \"\")\telse \"\"\n",
      "\tX1_data.append(this_color_features)\t\n",
      "\t#print img, len(train_lbp_features[img]), len(this_combined_features)\n",
      "#\n",
      "if args.pca:\n",
      "    X1_data = do_pca(X1_data)\n",
      "\n",
      "#\n",
      "X2_data = []\n",
      "for img in imagename_list:\n",
      "\tthis_texture_features = []\n",
      "\t# Features sobre 'texturas': histogramas de haraclick, lbp y surf\n",
      "\tthis_texture_features.extend(train_hara_features[img]) if (args.featuresHara != \"\")\telse \"\"\n",
      "\tthis_texture_features.extend(train_lbp_features[img])  if (args.featuresLbp != \"\")\telse \"\"\n",
      "\tthis_texture_features.extend(train_surf_features[img]) if (args.featuresSurf != \"\")\telse \"\"\n",
      "\t#\n",
      "\tX2_data.append(this_texture_features)\n",
      "\n",
      "if args.pca:\n",
      "    X2_data = do_pca(X2_data)\n",
      "\n",
      "for i in range(0, len(X1_data)):\n",
      "\tcombined_features = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\tcombined_features.extend(X1_data[i])\n",
      "\tcombined_features.extend(X2_data[i])\n",
      "\tX_data.append(combined_features)\n",
      "#\n",
      "print \"Training info loaded!\", len(X_data), len(Y_data)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Luego, se cargan las im\u00e1genes a evaluar y se le computan los atributos de acuerdo a los que se cargaron para el entrenamiento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#--------------------------------------------------------------------------------------\n",
      "image_targets = {}\n",
      "for f in glob.glob(args.trainingSetPath+'*.jpg'):\n",
      "\ttarget = 1 if 'cat' in f else 0\t\t\n",
      "\timagename = os.path.basename(f)\n",
      "\timage_targets[imagename] = target\n",
      "#\n",
      "#\n",
      "image_counter = 0\n",
      "surf_all_hist = {}\n",
      "gray_features = {}\n",
      "color_features = {}\n",
      "hara_features = {}\n",
      "lbp_features = {}\n",
      "surf_features = {}\n",
      "\n",
      "for imagename in image_targets:\n",
      "\timage_counter+=1\n",
      "\tfilename = args.trainingSetPath + imagename\n",
      "\tprint 'Processing image: ', image_counter, filename\n",
      "\n",
      "\t# Abro la imagen en escala de grises\n",
      "\timage = mahotas.imread(filename, as_grey=True)\n",
      "\n",
      "\tif (args.featuresGray != \"\"):\t# Extraer histograma de grises\n",
      "\t\tgray_hist, bins = np.histogram(image.flatten(), 256, [0, 256])\n",
      "\t\tgray_features[imagename] = gray_hist\n",
      "\t#\t\n",
      "\tif (args.featuresLbp != \"\"):\t# Extraer lbp (sobre la misma imagen del anterior, en escala de grises)\n",
      "\t\tradius = 3\n",
      "\t\tpoints = 4 * radius\t\t\t\t\t\t\t# Number of points to be considered as neighbourers\n",
      "\t\tlbp_hist = lbp.lbp(image, radius, points, ignore_zeros=False)\n",
      "\t\tlbp_features[imagename] = lbp_hist\n",
      "\t#\n",
      "\tif (args.featuresSurf != \"\"):\t# Extraer surf (sobre la misma imagen del anterior, en escala de grises)\n",
      "\t\tsurf_features = surf.surf(image)[:, 5:]\n",
      "\t\tsurf_all_hist[imagename] = surf_features\n",
      "\t#\t\t\t\n",
      "\t# Abro la imagen en colores\n",
      "\timage = mahotas.imread(filename, as_grey=False)\n",
      "\t#\n",
      "\tif (args.featuresColor != \"\"):\t# Extraer histograma de colores\n",
      "\t\tcolor_hist, bins = np.histogram(image.flatten(), 256, [0, 256])\n",
      "\t\tcolor_features[imagename] = color_hist\n",
      "\t#\n",
      "\tif (args.featuresHara != \"\"):\t# Extraer histograma haraclick (sobre la misma imagen del anterior, en colores)\n",
      "\t\thara_hist = mahotas.features.haralick(image).mean(0)\n",
      "\t\thara_features[imagename] = hara_hist\n",
      "\n",
      "#------------------------------------------------------------------------------------------------------------------\t\n",
      "if (args.featuresSurf != \"\"):\t# Clusterizo las features de 'surf'\n",
      "\tk = 300\n",
      "\tsurf_features = clusterSurfFeatures(surf_all_hist, k)\n",
      "#------------------------------------------------------------------------------------------------------------------"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Siguiendo la idea de combinar las features, se realiza el mismo proceso con las nuevas im\u00e1genes al realizado con el conjunto de entrenemiento."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "y_data = []\n",
      "x_data = []\n",
      "x1_data = []\n",
      "image_names = []\n",
      "for img in image_targets:\n",
      "\ttarget = 1 if 'cat' in img else 0\t\t\n",
      "\ty_data.append(target)\n",
      "\t#\n",
      "\tthis_color_features = []\n",
      "\t# Features sobre 'colores': histogramas de grises y colores \n",
      "\tthis_color_features.extend(train_color_features[img]) if (args.featuresColor != \"\")\telse \"\"\n",
      "\tthis_color_features.extend(train_gray_features[img])  if (args.featuresGray != \"\")\telse \"\"\n",
      "\tx1_data.append(this_color_features)\t\n",
      "\timage_names.append(img)\n",
      "#\n",
      "if args.pca:\n",
      "    x1_data = do_pca(x1_data)\n",
      "#\n",
      "x2_data = []\n",
      "for img in image_targets:\n",
      "\tthis_texture_features = []\n",
      "\t# Features sobre 'texturas': histogramas de haraclick, lbp y surf\n",
      "\tthis_texture_features.extend(train_hara_features[img]) if (args.featuresHara != \"\")\telse \"\"\n",
      "\tthis_texture_features.extend(train_lbp_features[img])  if (args.featuresLbp != \"\")\telse \"\"\n",
      "\tthis_texture_features.extend(train_surf_features[img]) if (args.featuresSurf != \"\")\telse \"\"\n",
      "\t#\n",
      "\tx2_data.append(this_texture_features)\n",
      "\n",
      "if args.pca:\n",
      "    x2_data = do_pca(x2_data)\n",
      "\n",
      "for i in range(0, len(x1_data)):\n",
      "\tcombined_features = []\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\tcombined_features.extend(x1_data[i])\n",
      "\tcombined_features.extend(x2_data[i])\n",
      "\tx_data.append(combined_features)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finalmente, se entrena un ensamble de clasificadores y se eval\u00faan las nuevas instancias. En este caso se utiliz\u00f3 una combinaci\u00f3n de 3 clasificadores de SK-Learn que se combinan en un esquema de votaci\u00f3n (VotingClassifier), a saber:\n",
      "\n",
      "<ul>\n",
      "<li> Random Forest (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
      "<li> Gradient Boosting (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n",
      "<li> Logistic Regression (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
      "</ul>\n",
      "\n",
      "Los par\u00e1metros en cada caso se obtuvieron con un esquema de Grid Search como se explic\u00f3 en el cap\u00edtulo 2. Respecto del uso de Support Vector Machines, se prefiri\u00f3 buscar una alternativa ya que, como se indica en la documentaci\u00f3n de la implementaci\u00f3n de SK-Learn (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), el tiempo para el ajuste es mayor que $N^2$ con lo que es dificil de escalar a mas de 10.000 instancias (con el kernel 'rbf' la clasificaci\u00f3n terminaba pero en un tiempo alto que dificultaba realizar m\u00faltiples pruebas).\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#clf = RandomForestClassifier()\n",
      "clf1 = RandomForestClassifier(max_depth=5, n_estimators=20, max_features=1)\n",
      "#clf1 = RandomForestClassifier(max_depth=100, n_estimators=100, max_features=10)\n",
      "#clf2 = SVC(kernel=\"linear\", C=0.025)\t#Kernel: linear, poly, rbf\n",
      "#clf2 = GradientBoostingClassifier(n_estimators=100)\n",
      "#clf3 = linear_model.LogisticRegression(C=1e5)\n",
      "clf2 = RandomForestClassifier(random_state=1)\n",
      "clf3 = RandomForestClassifier(random_state=2)\n",
      "\n",
      "print 'Testing... '\n",
      "\n",
      "eclf1 = VotingClassifier(estimators=[('rf', clf1), ('svm', clf2), ('gb', clf3)], voting='hard')\n",
      "eclf1 = eclf1.fit(X_data, Y_data)\n",
      "\n",
      "predictions = eclf1.predict(x_data)\n",
      "\n",
      "#----------------------------------------------------------------------------------------------------\n",
      "# Genero la salida para la competecia!\n",
      "fi = open(\"grupoXX.txt\", \"w\")\n",
      "for i in range(0, len(image_names)-1):\n",
      "\tprint image_names[i],\",\",predictions[i]\n",
      "\tfi.write(image_names[i]+\",\"+str(predictions[i])+\"\\n\")\n",
      "fi.close()\n",
      "#\n",
      "\n",
      "print classification_report(y_data, predictions)\n",
      "\n",
      "#predictions = clf.predict(X_test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3>Clasificaci\u00f3n de las nuevas instancias</h3>\n",
      "TBC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<h3> Conclusi\u00f3n</h3>\n",
      "TBC"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}