{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<h1> Trabajo Pr\u00e1ctico 1</h1>\n",
      "<h2>Aprendizaje Autom\u00e1tico - 2015</h2>\n",
      "<b>Matias Bonaventura y Gabriel Tolosa<br></b>\n",
      "mbonaventura@dc.uba.ar, tolosoft@unlu.edu.ar\n",
      "</center>                    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introducci\u00f3n\n",
      "\n",
      "El Aprendizaje Autom\u00e1tico (Machine Learning) es una disciplina dentro de las Ciencias de la Computaci\u00f3n que desarrolla t\u00e9cnicas y algoritmos para que las computadoras puedan llevar adelante diversas tareas complejas, mejorando su performance con la experiencia. \n",
      "\n",
      "Abarca m\u00faltiples \u00e1reas de estudio tales como la matem\u00e1tica y la estad\u00edstica, el dise\u00f1o, evaluaci\u00f3n y optimizaci\u00f3n de algoritmos, y la programaci\u00f3n eficiente, principalmente. Sus \u00e1reas de aplicaci\u00f3n abarcan un abanico muy amplio, ofreciendo m\u00e9todos que son utilizados en diversas ramas de las ciencias y la tecnolog\u00eda como por ejemplo en la biolog\u00eda, las finanzas, la rob\u00f3tica y - por supuesto - las ciencias de la computaci\u00f3n.\n",
      "\n",
      "En este trabajo se analizan y estudian tres enfoques para clasificaci\u00f3n de elementos que se basan en enfoques completamente diferentes. La clasificaci\u00f3n es un caso de Aprendizaje Supervisado en el cual se entrena un algoritmo (clasificador) con un conjunto de observaciones de diferentes clases o tipos y \u00e9ste construye una (o varias) hip\u00f3tesis posibles que ajusten tales observaciones y se selecione alguna (de acuerdo a diferentes criterios) que describa tales observaciones. Luego, a partir de contar con un modelo, se pretende que dicho clasificador pueda etiquetar o clasificar nuevas observaciones de acuerdo al modelo aprendido. La idea es poder maximizar una medida de performance y mejorarla conforme se cuente con nuevas instancias.\n",
      "\n",
      "A continuaci\u00f3n, se presentan los estudios y resultados sobre las siguientes estrategias de clasificaci\u00f3n, a saber:\n",
      "<pre>\n",
      "a) \u00c1rboles de Decisi\u00f3n\n",
      "b) Vecinos mas Cercanos\n",
      "c) Naive Bayes\n",
      "</pre>\n",
      "Cada uno se presenta en una notebook separada (ver las secciones que siguen con los links correspondientes), donde inclu\u00edmos una breve introducci\u00f3n al clasificador, detallse de los datos utilizados, las tareas y metodolog\u00edas realidas, y las conclusiones particulares a cada ejercicio.\n",
      "\n",
      "Como metodolog\u00eda general, se tomaron conjuntos de datos provistos por el equipo docente (o descargados de sitios sugeridos) y se realiz\u00f3 una implementaci\u00f3n utilizando la librer\u00eda Sk-Learn para el lenguaje Python. Para cada caso, se analiza en primer lugar las prestaciones globales para luego profundizar en posibles problemas de los que adolece cada uno de los enfoques."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1) \u00c1rboles de Decisi\u00f3n. Sobreajuste y ruido\n",
      "Se puede encontrar la secci\u00f3n que corresponde al primer ejercicio en el siguiente notebook:\n",
      "[Ejercicio 1: \u00c1rboles de Decisi\u00f3n. Sobreajuste y ruido](Ej1.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2) Vecinos M\u00e1s Cercanos. Sobreajuste y curse of dimensionality \n",
      "Se puede encontrar la secci\u00f3n que corresponde al segundo ejercicio en el siguiente notebook:\n",
      "[Ejercicio 2 - Vecinos M\u00e1s Cercanos. Sobreajuste y curse of dimensionality](Ej2.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3) Naive Bayes. Text Mining \n",
      " \n",
      "Se puede encontrar la secci\u00f3n que corresponde al tercer ejercicio en el siguiente notebook:\n",
      "[Ejercicio 3 - Clasificaci\u00f3n de Noticias por Naive Bayes](Ej3.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conclusiones\n",
      "\n",
      "En este trabajo se probaron diferentes algoritmos de aprendizaje para el problema de clasificaci\u00f3n. En particular, \u00c1rboles de Decisi\u00f3n, Vecinos M\u00e1s Cercanos y Naive Bayes. Aqu\u00ed se exponen conclusiones generales, no obstante, en cada ejercicio se realizan consideraciones particulares y se ofrecen mayores detalles.\n",
      "\n",
      "En el caso de los \u00e1rboles de decisi\u00f3n, la performance general es muy buena (> 0.9). Sin embargo, la construcci\u00f3n de un \u00e1rbol completo lleva a problemas de sobreajuste por lo que el control del crecimiento de los mismos es \u00fatil para permitir mayor poder de generalizaci\u00f3n. Esto se evidenci\u00f3 tmbi\u00e9n al introducir observaciones perturbadas en los datos.\n",
      "\n",
      "El enfoque por vecinos m\u00e1s cercanos logr\u00f3 clasificar correctamente el 80% de nuevas observaciones. Las pruebas con este enfoque fueron ajustar del tama\u00f1o de la vecindad y las funciones de peso para las distancias. El agregado de ruido (atributos con valores aleatorios) muestra c\u00f3mo se degrada la performance al crecer el nivel de perturbaci\u00f3n.\n",
      "\n",
      "Finalmente, el clasificador de noticias utilizando el modelo Naive Bayes obtiene muy buenas prestaciones cuando se cuenta con clases que poseen suficientes datos de entrenamiento, lo que le permite extraer mayor cantidad de t\u00e9rminos representativos de \u00e9stas. Las estrategias de reducci\u00f3n del vocabulario, en general, suben la exhaustividad de la recuperaci\u00f3n, degradando la precisi\u00f3n."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Ideas para Trabajos Futuros\n",
      "\n",
      "En todos los casos, existen m\u00faltiples par\u00e1metros a considerar para hacer el ajuste fino del clasificador. En general, se trata de una tarea experimental y dependiente del problema. Para el caso de los \u00e1rboles, una comparaci\u00f3n entre las estrategias que controlan la cantidad m\u00e1xima de hojas, la profundidad del \u00e1rbol o la cantidad m\u00ednima de \u00edtems en las hojas puede resultar interesante a los efectos de intentar obtener alguna regla a aplicar en diferentes problemas.\n",
      "\n",
      "Por el lado de los vecinos m\u00e1s cercanos, el estudio de las funciones de peso de la incidencia de los vecinos es igual de inter\u00e9s. De igual manera, se podr\u00eda estudiar el tradeoff entre dimensionalidad y cantidad de observaciones necesarias para obtener buenas prestaciones en el caso propuesto.\n",
      "\n",
      "Para el caso del clasificador de noticias, un estudio interesante consiste en tratar de determinar el m\u00ednimo conjunto de 'features' para un problema dado que permita realizar, por ejemplo, tareas de clasificaci\u00f3n sobre flujos en tiempo real."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Referencias\n",
      "-----------\n",
      "<ul>\n",
      "<li>Manning, C.D.; Raghavan P. and Schuetze, H. Introduction to Information Retrieval. Cambridge University Press. 2008.\n",
      "<li>Marsland, S. Machine Learning: An Algorithmic Perspective. 2nd. Edition. Chapman & Hall/Crc Machine Learning & Pattern Recognition. 2014.\n",
      "<li>McCallum A. and Nigam, K. A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization. 1998\n",
      "<li> Metsis, V; Androutsopoulos, I and Paliouras, G. Spam filtering with naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS). 2006.\n",
      "<li>Mitchell, T. M. Machine learning. WCB. 1997.\n",
      "<li>Shakhnarovich, G.; Indyk, P.; and Darrell, T. Nearest-neighbor methods in learning and vision: theory and practice. The MIT Press. 2006.\n",
      "</ul>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}